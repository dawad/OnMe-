{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dawad/OnMe-/blob/master/Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLoOxkmOhVWD",
        "colab_type": "code",
        "outputId": "c402a49d-7bcd-486a-fa2b-e08bd7ecca63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "import tensorflow as tf \n",
        "import pathlib \n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import csv \n",
        "import json\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd3t4oDYkFYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def extract images from zip files\n",
        "import zipfile\n",
        "def extract_file():\n",
        "  zip_ref = zipfile.ZipFile(\"/content/drive/My Drive/assignment_imgs.zip\", 'r')\n",
        "  zip_ref.extractall(\"/content/drive/My Drive/\")\n",
        "  zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfXGTd_L9C9I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read csv file\n",
        "def read_csv_file(path_file) :\n",
        "  classes_id =[]\n",
        "  with open(path_file) as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for row in csv_reader:\n",
        "      for field in row:\n",
        "        if 'tomat' in field:\n",
        "          classes_id.append(row[0])\n",
        "  return classes_id\n",
        "\n",
        "# read json_file\n",
        "def read_json_file(path_file, classes_id):\n",
        "  imges_files =[]\n",
        "  with open(path_file) as json_file:\n",
        "    data=json.load(json_file)\n",
        "    for img_id, value in data.items():\n",
        "      if value[0]['id'] in classes_id:\n",
        "        imges_files.append(img_id)\n",
        "  return imges_files "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hHE846dzSaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# step 3: parse every image in the dataset using `map`\n",
        "def _parse_function(filename, label):\n",
        "    image_string = tf.io.read_file(filename)\n",
        "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
        "    image = tf.image.resize(image, [224, 224])\n",
        "    return image, label\n",
        "\n",
        "def show(image, label):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(label.numpy().decode('utf-8'))\n",
        "  plt.axis('off')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amaEDfAsG_4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get model of deep learning from tensorflow \n",
        "def AlexNet(IMG_HEIGHT, IMG_WIDTH):\n",
        "  model = models.Sequential()\n",
        "  # first convolution layer\n",
        "  model.add(layers.Conv2D(filters=96, kernel_size=(11, 11), strides=(4, 4), padding='valid',\n",
        "                          input_shape=(IMG_HEIGHT, IMG_WIDTH ,3)))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2,2), padding='valid'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  # second convolution layer\n",
        "  model.add(layers.Conv2D(filters=256, kernel_size=(11,11), strides=(1,1),\n",
        "                          padding='valid'))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  # third convolution layer\n",
        "  model.add(layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  # 4th convolution layer\n",
        "  model.add(layers.Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  #5th convolutional layer \n",
        "  model.add(layers.Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid'))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(4096, input_shape=(224*224*3,)))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dropout(0.4))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(4096))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dropout(0.4))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(1000))\n",
        "  model.add(layers.Activation('relu'))\n",
        "  model.add(layers.Dropout(0.4))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.Dense(2))\n",
        "  model.add(layers.Activation('softmax'))\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QbXD2woRKX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnDwPfNYAdgg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_images, labels):\n",
        "  # step 1: create a dataset returning slices of `filenames`\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((tf.constant(train_images), tf.constant(labels)))\n",
        "  dataset = dataset.map(_parse_function)\n",
        "  dataset = dataset.batch(2)\n",
        "\n",
        "  # step 2: create iterator and final input tensor\n",
        "  iterator = dataset.make_one_shot_iterator()\n",
        "  images_files , labels_id = iterator.get_next()\n",
        "  epochs = 10\n",
        "  batch_size = 128\n",
        "  keep_probability = 0.7\n",
        "  learning_rate = 0.001\n",
        "  # step 3: compile AlexNet \n",
        "  alexmodel= AlexNet(224,224)\n",
        "  alexmodel.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  # step 4: train AlexNet \n",
        "  history = alexmodel.fit(images_files , labels_id,batch_size=64, epochs=1, verbose=1, \n",
        "                          shuffle=True, steps_per_epoch=1000)\n",
        "  # step 5: save the model\n",
        "  alexmodel.save('alexmodel.h5')\n",
        "  return history, alexmodel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJYGiUX3DmHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# draw accuracy and loss \n",
        "def draw_acc_loss(history):\n",
        "  history_dict = history.history\n",
        "  acc = history_dict['acc']\n",
        "  epochs = range(1, len(acc) + 1)\n",
        "  plt.plot(epochs, acc, 'b', label='accuracy')\n",
        "  loss = history_dict['loss']\n",
        "  plt.plot(epochs, loss, 'b', label='loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfljNTkYLPN0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "756136ed-57ec-4b2a-d541-724a122f9f59"
      },
      "source": [
        "# prepare dataset\n",
        "classes_id = read_csv_file('/content/drive/My Drive/label_mapping.csv')\n",
        "imges_files = read_json_file('/content/drive/My Drive/img_annotations.json', classes_id)\n",
        "images = os.listdir('/content/drive/My Drive/assignment_imgs')\n",
        "train_images =[]\n",
        "labels =[]\n",
        "for img in images : \n",
        "  if img in imges_files:\n",
        "    labels.append(1)\n",
        "  else: \n",
        "    labels.append(0)\n",
        "  train_images.append(os.path.join('/content/drive/My Drive/assignment_imgs', img))\n",
        "print(len(train_images), len(labels))\n",
        "# train model \n",
        " mod_history, alexmodel = train(train_images, labels)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3000 3000\n",
            "Train on 1000 samples\n",
            "1000/1000 [==============================] - 920s 920ms/step - loss: 0.1809 - acc: 0.9610\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-9f4fc9122189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmod_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malexmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# draw accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mhistory_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euDGq3XCXZHw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "1aaccc44-59f2-4e8e-f943-9f820f66afc6"
      },
      "source": [
        "# test model \n",
        "new_model = tf.keras.models.load_model('alexmodel.h5')\n",
        "path_img = '/content/drive/My Drive/assignment_imgs/ffabf3df9937ea8716c817cacac27346.jpeg'\n",
        "prediction = has_tomato(path_img, new_model)\n"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-580e8088c6b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'alexmodel.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/My Drive/assignment_imgs/ffabf3df9937ea8716c817cacac27346.jpeg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhas_tomato\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-996edcd8398a>\u001b[0m in \u001b[0;36mhas_tomato\u001b[0;34m(filename, alexmodel)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m224\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malexmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediciton\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prediciton' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVoBiPnVaJGJ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9HUEYhzaIfQ",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2kSaJjQPNmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def has_tomato(filename, alexmodel):\n",
        "  image_string = tf.io.read_file(filename)\n",
        "  image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
        "  image = tf.image.convert_image_dtype(image_decoded, tf.float32)\n",
        "  test_image = tf.image.resize(image, [224, 224])\n",
        "  predictions = alexmodel.predict(tf.reshape(test_image, [1,224,224, 3]), steps=10)\n",
        "  print(prediction)\n",
        "  return np.argmax(predictions[0])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}